{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import quicklens as ql\n",
    "\n",
    "import utilities \n",
    "import flow_architecture\n",
    "import training_data\n",
    "\n",
    "import base64\n",
    "import io\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import quicklens as ql\n",
    "import scipy.ndimage\n",
    "import torch\n",
    "import torch.fft\n",
    "print(f'TORCH VERSION: {torch.__version__}')\n",
    "import importlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importlib.reload(training_data)\n",
    "importlib.reload(utilities)\n",
    "#importlib.reload(flow_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  torch_device = 'cuda'\n",
    "  float_dtype = np.float32 # single\n",
    "  torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "  torch.cuda.set_device(1) # Set to a GPU\n",
    "else:\n",
    "  torch_device = 'cpu'\n",
    "  float_dtype = np.float64 # double\n",
    "  torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(f\"TORCH DEVICE: {torch_device} {torch.cuda.current_device()}\")\n",
    "\n",
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamode = \"nbodybased\" #gaussbased or nbodybased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if datamode == \"gaussbased\":\n",
    "    nx = 64 #16 # number of pixels.\n",
    "    dx = 4.*utilities.d2r / float(nx)   #1 deg for 16px. 4 deg for 64 px\n",
    "    fnl = 0.2 #0.05\n",
    "    fnlmode = True\n",
    "    trainingdata = training_data.TrainingDataGaussBased(nx,dx,fnl,fnlmode)\n",
    "    #trainingdata = training_data.TrainingDataGaussBased_Cached(nx,dx,fnl,fnlmode,nmaps_train=10000,nmaps_valid=10000)\n",
    "    lmax = trainingdata.lmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if datamode == \"nbodybased\":\n",
    "    trainingdata = training_data.TrainingDataNbodyBased()\n",
    "    nx = trainingdata.nx\n",
    "    dx = trainingdata.dx\n",
    "    lmax = trainingdata.lmax\n",
    "    fnl = trainingdata.fnl #arbitrary normalisation in nbody case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the norm\n",
    "samples_test = trainingdata.draw_samples_of_px(1000)\n",
    "print (np.std(samples_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# defining the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lattice_shape = (nx,nx)\n",
    "\n",
    "priormode = \"correlated\" #\"correlated\" #correlated or whitenoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if priormode == \"whitenoise\":\n",
    "    prior = flow_architecture.SimpleNormal(torch.zeros(lattice_shape), torch.ones(lattice_shape))\n",
    "    \n",
    "if priormode == \"correlated\":\n",
    "    rfourier_shape = (nx,int(nx/2+1),2)\n",
    "    prior = flow_architecture.CorrelatedNormal(torch.zeros(rfourier_shape), torch.ones(rfourier_shape),nx,dx,trainingdata.cl_theo,torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nsamples = 100\n",
    "torch_z = prior.sample_n(nsamples)\n",
    "z = utilities.grab(torch_z)\n",
    "torch_logp = prior.log_prob(torch_z)\n",
    "logp = utilities.grab(torch_logp)\n",
    "#print(f'z.shape = {z.shape}')\n",
    "\n",
    "fig, ax = plt.subplots(2,4, figsize=(10,5))\n",
    "for j in range(4):\n",
    "    for i in range(2):\n",
    "        ind = i*4 + j\n",
    "        ax[i,j].imshow(z[ind], cmap='viridis')\n",
    "        ax[i,j].axes.xaxis.set_visible(False)\n",
    "        ax[i,j].axes.yaxis.set_visible(False)\n",
    "        #print (\"logp unnormed\",logp[ind])\n",
    "plt.show()\n",
    "\n",
    "print (np.std(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_layers = 16 #std: 16 \n",
    "hidden_sizes = [16,16] #std: [16,16] \n",
    "kernel_size = 3\n",
    "layers = flow_architecture.make_flow1_affine_layers(\n",
    "    lattice_shape=lattice_shape, n_layers=n_layers, \n",
    "    hidden_sizes=hidden_sizes, kernel_size=kernel_size,torch_device=torch_device)\n",
    "model = {'layers': layers, 'prior': prior}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = 0.001 #standard: 0.001\n",
    "optimizer = torch.optim.Adam(model['layers'].parameters(), lr=base_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_era = 10000\n",
    "N_epoch = 100\n",
    "batch_size = 128\n",
    "print_freq = N_epoch\n",
    "plot_freq = 1\n",
    "\n",
    "history = {\n",
    "    'loss' : [],\n",
    "}\n",
    "\n",
    "lossList = []\n",
    "validationList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, metrics, trainingdata):\n",
    "    layers, prior = model['layers'], model['prior']\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x = trainingdata.draw_samples_of_px(batch_size)\n",
    "    x = torch.from_numpy(x).float().to(torch_device)\n",
    "\n",
    "    u, log_pu, log_J_Tinv = flow_architecture.apply_reverse_flow_to_sample(x, prior, layers)\n",
    "\n",
    "    loss = -(log_pu + log_J_Tinv).mean()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    lossval = utilities.grab(loss)\n",
    "    lossList.append(lossval)\n",
    "    \n",
    "    metrics['loss'].append(lossval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pretrained = False\n",
    "\n",
    "print(\"  era  |     sample loss      |   validation loss   |  time\")\n",
    "print(\"--------------------------------------------------------------\")\n",
    "if not use_pretrained:\n",
    "    for era in range(N_era):\n",
    "        print (\"  \",era,' ','|','                    'if era==0 else lossList[-1],'|','                    'if era==0 else validationList[-1],'|', round(time.time()-startTime), \"s\")\n",
    "        for epoch in range(N_epoch):\n",
    "            train_step(model, optimizer, history, trainingdata)\n",
    "            \n",
    "            v,_ = draw_samples_of_v(batch_size)\n",
    "            v = torch.from_numpy(v).float().to(torch_device)\n",
    "            u_v, log_pu_v, log_J_Tinv_v = flow_architecture.apply_reverse_flow_to_sample(v, prior, layers)\n",
    "            validation = -(log_pu_v + log_J_Tinv_v).mean()\n",
    "            validationval = utilities.grab(validation)\n",
    "            validationList.append(validationval)\n",
    "else:\n",
    "    print (\"restoring state dict.\")\n",
    "    save_model_dir = \"models/gauss_cached15/\"\n",
    "    model['layers'].load_state_dict(torch.load(save_model_dir+\"model\"))\n",
    "    with open(save_model_dir+'train_loss.pkl', 'rb') as f:\n",
    "        lossList = pickle.load(f)\n",
    "    with open(save_model_dir+'val_loss.pkl', 'rb') as f:\n",
    "        validationList = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "idmin = 0\n",
    "idmax = len(lossList)\n",
    "\n",
    "lossListmod = np.array(lossList.copy())\n",
    "#lossListmod[lossListmod>-1800] = -2000\n",
    "\n",
    "validationListmod = np.array(validationList.copy())\n",
    "#validationListmod[validatonListmod>-1800] = -2000\n",
    "#validationListmod = validationListmod[validationListmod<0]\n",
    "\n",
    "fig=plt.figure(figsize=(5,3))\n",
    "#plt.plot(np.arange(idmin,len(validationListmod),1),validationListmod[idmin:idmax],label='Validation')\n",
    "plt.plot(np.arange(idmin,idmax,1),lossListmod[idmin:idmax],color='red',label='Training')\n",
    "plt.legend(loc=1,frameon=False,fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Batch',fontsize=14)\n",
    "plt.ylabel('Loss',fontsize=14)\n",
    "fig.tight_layout()\n",
    "plt.savefig('loss_nbody.pdf') #loss_gauss.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visually inspect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "u, log_pu, z, log_pz = flow_architecture.apply_flow_to_prior(prior, layers, batch_size=1)\n",
    "u_rev, log_pu_rev, log_J_Tinv = flow_architecture.apply_reverse_flow_to_sample(z, prior, layers)\n",
    "#print (z)\n",
    "#print (u)\n",
    "#print (u_rev)\n",
    "print (log_pu)\n",
    "print (log_pu_rev)\n",
    "print (log_pz)\n",
    "print (log_J_Tinv)\n",
    "print (log_pu+log_J_Tinv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "u, log_pu, z, log_pz = flow_architecture.apply_flow_to_prior(prior, layers, batch_size=16)\n",
    "z = utilities.grab(z)\n",
    "u = utilities.grab(u)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(9,3.5))\n",
    "for i in range(3):\n",
    "    ax[i].imshow(u[i], cmap='viridis')\n",
    "    ax[i].axes.xaxis.set_visible(False)\n",
    "    ax[i].axes.yaxis.set_visible(False)\n",
    "fig.suptitle('Prior samples', fontsize=14)\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"samples_nbody_prior.pdf\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(9,3.5))\n",
    "for i in range(3):\n",
    "    ax[i].imshow(z[i], cmap='viridis')\n",
    "    ax[i].axes.xaxis.set_visible(False)\n",
    "    ax[i].axes.yaxis.set_visible(False)\n",
    "fig.suptitle('Model samples', fontsize=14)\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"samples_nbody_model.pdf\")\n",
    "plt.show()\n",
    "\n",
    "x = trainingdata.draw_samples_of_px(16)\n",
    "fig, ax = plt.subplots(1,3, figsize=(9,3.5))\n",
    "for i in range(3):\n",
    "    ax[i].imshow(x[i], cmap='viridis')\n",
    "    ax[i].axes.xaxis.set_visible(False)\n",
    "    ax[i].axes.yaxis.set_visible(False)\n",
    "fig.suptitle('Target samples', fontsize=14)\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"samples_nbody_target.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make power spectrum and density plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntest = 10000\n",
    "batchsize_test = 100\n",
    "samples_true = np.zeros( (ntest,nx,nx) )\n",
    "samples_true_logp_true = np.zeros( (ntest) )\n",
    "samples_flow = np.zeros( (ntest,nx,nx) )\n",
    "samples_prior = np.zeros( (ntest,nx,nx) )\n",
    "\n",
    "#draw truth samples\n",
    "for batch_id in range(100):\n",
    "    z,log_p = trainingdata.draw_samples_of_pv(batchsize_test)\n",
    "    #print (z.shape,log_p.shape)\n",
    "    samples_true[batch_id*batchsize_test:(batch_id+1)*batchsize_test] = z  \n",
    "    samples_true_logp_true[batch_id*batchsize_test:(batch_id+1)*batchsize_test] = -0.5*log_p\n",
    "\n",
    "#make flow samples\n",
    "for batch_id in range(100):\n",
    "    u, log_pu, z, log_pz = flow_architecture.apply_flow_to_prior(prior, layers, batch_size=batchsize_test)\n",
    "    z = utilities.grab(z)\n",
    "    u = utilities.grab(u)\n",
    "    samples_flow[batch_id*batchsize_test:(batch_id+1)*batchsize_test] = z\n",
    "    \n",
    "#make gaussianized samples\n",
    "for batch_id in range(100):\n",
    "    torch_z = prior.sample_n(nsamples)\n",
    "    z = utilities.grab(torch_z)\n",
    "    samples_prior[batch_id*batchsize_test:(batch_id+1)*batchsize_test] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbins      = np.linspace(100, lmax, 25) # multipole bins.\n",
    "\n",
    "cl_avg_true,ell_binned = utilities.estimate_ps_ensemble(samples_true,nx,dx,lbins)\n",
    "cl_avg_flow,ell_binned = utilities.estimate_ps_ensemble(samples_flow,nx,dx,lbins)\n",
    "cl_avg_prior,ell_binned = utilities.estimate_ps_ensemble(samples_prior,nx,dx,lbins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "fig=plt.figure(figsize=(5,3))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(ell_binned,cl_avg_true*ell_binned**2.,color='red',label='$C_\\ell^{true}$')\n",
    "ax.plot(ell_binned,cl_avg_flow*ell_binned**2.,color='black',label='$C_\\ell^{flow}$')\n",
    "ax.plot(ell_binned,cl_avg_prior*ell_binned**2.,color='green',ls='dotted',label='$C_\\ell^{prior}$')\n",
    "#ax.plot(cl_theo_ell,cl_theo_tt*cl_theo_ell**2.,color='black',label='$C_\\ell^{theo}$')\n",
    "plt.legend(loc=1,frameon=False,fontsize=14)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(0,trainingdata.lmax-600)\n",
    "plt.xlabel('$\\ell$',fontsize=14)\n",
    "plt.ylabel('$\\ell^2 C_\\ell$',fontsize=14)\n",
    "ax.grid(True)\n",
    "fig.tight_layout()\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "#plt.savefig('ps_nobdy_128.pdf')\n",
    "plt.savefig('ps_nbody.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# non-gaussianity estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw some gaussian samples for variance calculation\n",
    "samples_true_gauss = np.zeros( (ntest,nx,nx) )\n",
    "\n",
    "#draw truth samples\n",
    "for batch_id in range(100):\n",
    "    z,log_p = trainingdata.draw_samples_of_pv_gauss(batchsize_test)\n",
    "    samples_true_gauss[batch_id*batchsize_test:(batch_id+1)*batchsize_test] = z  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"FNL LOCAL\")\n",
    "cl_theo_normed = trainingdata.cl_theo_normed \n",
    "fnls_flow,_ = utilities.estimate_fnl_local_ensemble(samples_flow, samples_true_gauss, cl_theo_normed,nx,dx)\n",
    "fnls_true,_ = utilities.estimate_fnl_local_ensemble(samples_true, samples_true_gauss, cl_theo_normed,nx,dx)\n",
    "\n",
    "print(\"TRUTH: mean fnl local\",np.mean(fnls_true),\"std per map\",np.std(fnls_true), \"snr ratio\", np.mean(fnls_true)/np.std(fnls_true))\n",
    "print(\"FLOW: mean fnl local\",np.mean(fnls_flow),\"std\",np.std(fnls_flow), \"snr ratio\", np.mean(fnls_flow)/np.std(fnls_flow))\n",
    "print (\" \")\n",
    "renormfactor = fnl/np.mean(fnls_true) #takes into account ng variance\n",
    "print(\"TRUTH renorm: mean fnl local\",np.mean(fnls_true)*renormfactor,\"std per map\",np.std(fnls_true)*renormfactor, \"snr ratio\", np.mean(fnls_true)/np.std(fnls_true))\n",
    "print(\"FLOW renorm: mean fnl local\",np.mean(fnls_flow)*renormfactor,\"std per map\",np.std(fnls_flow)*renormfactor, \"snr ratio\", np.mean(fnls_flow)/np.std(fnls_flow))\n",
    "\n",
    "#print(\"check zero val: mean no-fnl local\",np.mean(fnl_normed_gauss),\"std\",np.std(fnl_normed_gauss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"FNL EQUILATERAL\")\n",
    "fnls_flow,_ = utilities.estimate_fnl_equilateral_ensemble(samples_flow, samples_true_gauss, trainingdata.cl_theo_ell,nx,dx)\n",
    "fnls_true,_ = utilities.estimate_fnl_equilateral_ensemble(samples_true, samples_true_gauss, trainingdata.cl_theo_ell,nx,dx)\n",
    "\n",
    "print(\"TRUTH: mean fnl equilateral\",np.mean(fnls_true),\"std per map\",np.std(fnls_true), \"snr ratio\", np.mean(fnls_true)/np.std(fnls_true))\n",
    "print(\"FLOW: mean fnl equilateral\",np.mean(fnls_flow),\"std per map\",np.std(fnls_flow), \"snr ratio\", np.mean(fnls_flow)/np.std(fnls_flow))\n",
    "print (\" \")\n",
    "print(\"TRUTH renorm: mean fnl equilateral\",np.mean(fnls_true)*renormfactor,\"std per map\",np.std(fnls_true)*renormfactor, \"snr ratio\", np.mean(fnls_true)/np.std(fnls_true))\n",
    "print(\"FLOW renorm: mean fnl equilateral\",np.mean(fnls_flow)*renormfactor,\"std per map\",np.std(fnls_flow)*renormfactor, \"snr ratio\", np.mean(fnls_flow)/np.std(fnls_flow))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
